%!TEX root = /Users/jp/Thesis/Thesis.tex
\chapter{Tools} % (fold)
\label{cha:tools}

\section{Introduction} % (fold)
\label{sec:introduction_tools}
While the main topic of this research was the development of a query analysis tool for multi-domain queries, several tools were developed and used on the side in order to support the research. In order to gather and analyze data in the form of natural language questions, two tools were developed, to filter and process the data. In addition, two external parsers were evaluated in order to find an optimal strategy.
% section introduction_tools (end)

\section{Data Gathering \amper\ Analysis} % (fold)
\label{sec:data_gathering_and_analysis}

In order to grasp the intricacies of the problem of parsing natural language queries in our particular domain, we tried to look for a source of a large number of already-made queries that correspond to our criteria, that is that they span multiple domains and that they are expressed naturally. We found Yahoo! Answers to be a good source for such kind of data, albeit with a very low signal to noise ratio. That is, a first step on the way to obtaining a valuable corpus would be to extract and filter the data from this site. Thankfully, Yahoo! Answers provide an Application Programming Interface that allows us to obtain questions in large quantities in a XML or JSON format. We thus created a Web Tool, code-named Siphon, that extracts the questions from Yahoo! Answers and present them to the user where he or she can decide whether to accept or reject each entry.

\begin{huge}
  Insert Screenshot
\end{huge}

This tool presents a simple list view where, by default, the yet unfiltered entries are shown. Through the use of keyboard shortcuts or the provided links, the user can circle through the entries, accepting or rejecting them. The entries are stored permanently, whether accepted or not, and it is then possible to change mode and go look at the previously accepted or rejected entries, before coming back to filter new ones. The tool will load new entries from Yahoo! Answers as the number of unfiltered entries get low, insuring a steady amount of entries to filter.

Internally, this data is stored to a SQLite database and is shown using the Merb web framework, with the use of the jQuery framework to enhance the site with Javascript.

Another interesting functionality of this tool is the ability to inspect individual entries, giving a view of how they would be parsed using both the shallow parser and the Stanford parser, which shall be described further. Parsing a large corpus such as this one is not a trivial matter, and so, a second tool was developed to offload and distribute the parsing tasks to diverse computers. This tool, called Bee, uses a simple queue service as a central dispatching mechanism and connects to a document database to store the results. Its concept is centered around tasks, defined by the developer, which are run by workers waiting for messages on a pre-defined queue. In answer to these parameterized messages, the worker will launch the task and store its result on the database. On the other side, we have the Siphon tool posting messages to this queue as new entries are created, and using the results from the document database to show to the user.

The queue used is Kestrel, which is a simple open-source queue implementation in Scala reusing the now common Memcache protocol, and which has proved its stability and performance at Twitter Inc. where it was developed. As we could expect the schema of the documents to change often as new tasks are added to the service, a schema-less database was chosen. Given the breadth of technology used, the Apache CouchDB project was used as it internally keeps data in the JSON format which has a wide support across different languages. The data structures available in JSON, like objects and arrays, were an essential asset for the transcoding of the parse trees to a common format. The Bee software itself is also written in Scala, providing an easy object-oriented concurrency model based on the actors concept.

In the spirit of open source and given the strong use of it during the development of this project, the two tools described here are made available as free and open source software and can be found at www.github.com/jpbougie.

% section data_gathering_and_analysis (end)

\section{Parsers} % (fold)
\label{sec:parsers}

% section parsers (end)
% chapter tools (end)