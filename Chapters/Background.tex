%!TEX root = /Users/jp/Thesis/Thesis.tex
\chapter{Background} % (fold)
\label{cha:background}

\section{Definitions} % (fold)
\label{sec:definitions}

Before getting into deeper details about the project, it is important to clearly define some terms that will be used throughout this report.

\subsection{Web Service} % (fold)
\label{sub:web_service_def}

We think of Web Services in an abstract sense, as providers of content in response to a request, over the Internet. We assume that they can answer a precise query based on a set of defined conditions, and that they will return an ordered set of answers, the order being either based on a criteria provided by the user or by a relevancy score internal to the service. We can further take in consideration that the service has some performance rating, depending on the delay in which it returns the results as well as the quantity and quality of the results. Furthermore, we expect most web services to support some kind of pagination protocol in order to retrieve subsets of the data sequentially. Another important aspect of the services is that they might be subject to a monetary cost, or a limit in the querying rate, based on a daily period or otherwise.

For our purpose, the technology used for the transport and querying interface doesn't matter, be it SOAP, REST, Protobuffers or another binary or text encoding protocol. We assume that an adapter will be developed for each type of Web Service in order to use it.

% subsection web_service_def (end)

\subsection{Domain} % (fold)
\label{sub:domain_def}

A domain is a field of research as provided by one or more search engines. It could be, for example, international events, health professionals, countries information. In a query made by a user, a domain is characterized by the following parts:

\begin{itemize}

  \item One or more objects: While a domain represent an abstract entity, these are the concrete apparitions of the domain within the query. They may correspond directly to the name of the domain (eg. Give me a warm \emph{country}\ with cheap flights on the weekend of the first of may) or not (eg. Give me a \emph{conference}\ about computer science where I can also go to the opera, where conference is an object of the domain ``Event''). Additionally, more than one objects might refer to the same domain, in which case they need to be taken together.

  \item Conditions: Some conditions are given explicitly (eg. Give me a conference \emph{about computer science}.), while others can be deducted from the objects (\emph{conference}\ being a type of event). In general, conditions restrict the field of search that will be sent to the service. Some conditions are objective while others make use of subjective judgements (eg. \emph{warm}\ country).

  \item Ordering: Making the ranking and ordering according to some relevance score is an integral part of this project, and thus the ordering must be extracted and taken into account while answering a user's query. Again, some conditions can be subjective (the \emph{best}\ doctor in the region) while other elements can be taken both as a condition and as an implicit ordering ( \emph{cheap}\ flights will expose a cutoff in addition to the ordering by ascending price).

  \item One or more joins with other domains: at the core of this project is the presence of multiple domains within one query. These different domains must be joined in some way. These joins can be explicit, or inferred through a chain of relationships (an event is an city, that city is in a country) as expressed through a semantic network.

\end{itemize}


% subsection domain_def (end)

% section definitions (end)


\section{SeCo -- Search Computing} % (fold)
\label{sec:seco}

SeCo is the platform which aims to push the limits of the field of multi-domain queries by formalizing theoretical aspects as well as providing a software engineering point of view, enabling the construction of a usable search engine that will answer arbitrary queries.

\subsection{General Architecture} % (fold)
\label{sub:general_architecture}

SeCo is divided in different higher-level components orchestrated in a service-oriented manner. The main components are the query analysis, the query-to-domain mapper, the query planner, the query engine and the results transformation. Two frameworks named the service and domain frameworks are also added as intelligent repositories.

A query sent from the user first passes through the query analysis and the query-to-domain mapper, where the different domains and properties are extracted from the natural language query. It then goes to the query planner, which creates an execution plan taking into accounts the different costs associated to executing the query, in order to create the most efficient execution. The different subqueries are then sent to the domain and service frameworks, which take care of calling the external services through a Web or messaging interface. The results are then collected, and, according to the plan, merged back together. The final results are then transformed before being sent back to the user.

% subsection general_architecture (end)

\subsection{Query Analysis} % (fold)
\label{sub:query_analysis}

In order for a user to query the database of information and knowledge available in SeCo, he or she first needs to input it, whether by a Web interface, or by making a direct call to the programming interface of the system. The type of query the user gives can vary a lot, and it is for the system to try and understand the meaning of the question. In general, the contract of this component is to take the input as given by the user, try to separate it according to the sub-queries that can be found and send those down the line to the query planner.

% subsection query_analysis (end)

\subsection{Query to Domain Mapper} % (fold)
\label{sub:query_to_domain_mapper}

While the set of sub-queries is a first step towards actually executing the query, it still represents an abstract, human request that might or might not correspond to the the services available in the system. The Query to Domain Mapper will take each sub-query in turn, and corresponding to the vocabulary used and the elements in the query, will try to associate a domain as stored in the domain framework. It will send this augmented request to the next component, the query planner.

% subsection query_to_domain_mapper (end)

\subsection{Query Planner} % (fold)
\label{sub:query_planner}

The Query Planner's task is to take the set of high-level queries to different domains and turn it into an executable plan. This involves planning the fetching of data to web services, taking into account the frequency and quantity of data that will be required, and the merge operations that will be done, all in order to minimize the time and memory spent, as well as any costs related to the user of the web services.

This plan is now ready to be executed by the query engine as it contains precise instructions on what to execute and in which order. It is comparable to having taken a declarative program as input and obtaining an imperative program that can be run directly.

% subsection query_planner (end)

\subsection{Query Engine} % (fold)
\label{sub:query_engine}
This component takes the low-level plan from the query planner and executes the different service calls in parallel, merging and ordering when required. It will return the final results of the query in a pure and internal format as they become available, sending them to the results transformation component for their final processing.

Care must be taken in accord to the availability and performance of the invoked services. Since there is an heavy reliance on external components, failures or low performance problems will be common and will have to be considered while fetching the results.
% subsection query_engine (end)

\subsection{Results Transformation} % (fold)
\label{sub:results_transformation}
This component's role is to take the results as given by the Query Engine, and to transform and format them in the way that was required by the client. It implements features such as paging, XML or XHTML output.
% subsection results_transformation (end)

% section seco (end)

\section{Sources of Input Data} % (fold)
\label{sec:sources_of_input_data}

In order to get some interesting results and also to test the validity of our approach, a corpus of example data was required. Given the originality of the project, there was no available corpus that is both already filtered and annotated according to our needs. While there are corpora that include single-domain tagging, or simple text annotations, none were specific enough to include multi-domain questions highlighted with the parts of the sentences that correspond to diverse domains and that include the domains related to each question.

We thus had to go find a large and open source of questions that could possibly be asked by end users and that could eventually be automatically translated into the format needed by the query engine. One such source was found to be Yahoo! Answers, a service where users can input their questions about diverse subjects and have other users directly and personally answer. While they do not give a complete access to their corpus, they provide, free of charge, a Web application programming interface that allows the gradual extraction of the questions.

Questions in Yahoo! Answers are organized in categories representing different areas of human interests, like technology, spirituality, family matters. In a first pass, we surveyed many of these categories, but we found the \emph{tourism}\ category to contain the overwhelming majority of interesting questions, and thus it was decided to take it as the sole sample, as the others categories had a very low signal to noise ratio.

This brings us to the problem of filtering this data. Given that it is undirected, almost random, user input, the quality of the questions asked was varying. This quality we are looking for in a question is quite abstract and thus has had in a first pass to be decided by a human being. The criteria we apply to the filtering of the questions were manifold. The first one was a rough judgement of the quality of the English; while we cannot expect users to write perfect English, the tools we use in the processing chain are based on annotations of published text and thus are not trained on mistaken input. In the engineering phase, we can hope to expand the training of the data to include malformed sentences.

Furthermore, it is to be understood that users of Yahoo! Answers get to divide their question into two different input fields, a first one containing a brief question, as well as a longer and more descriptive version of their asking. For the sake of brevity and to represent a typical question that will be input in the prototypical search box, we decided to take only the smaller version of the question as our input. As such, we also need to remove entries where user chose to input meaningless or generic data in the summary (e.g. ``Help please!'').

The final and most elusive criterion is the presence of a multi-domain aspect to the question. It is arguable, but it could be assumed that most users, trained for years by the mainstream search engines to ask very precise questions in order to get somewhat relevant results, are now used to ask questions in a single-domain manner, then trying to make the links themselves, even when they do ask other users. It is thus difficult to extract true multi-domain questions from the existing corpus. Furthermore, while a multi-domain question is clearly defined in terms of querying more than one service, the quality of a question to span more than one domain from the point of view of natural language is fuzzier. In particular, the difference between a single-domain question accompanied by a complex condition or a true multi-domain question can at times be confusing.

To compound these diverse problems, we went from a simple yes/no filtering scheme to a more complex range of rating, where an entry is annotated with a single score which can range from 1 to 5, giving us some flexibility in organizing the results, as well as covering more possible input cases than could be extracted previously.

While this concludes the discussion on our main source of input data, further sources were used. The first of these is WordNet, the most comprehensive lexical database available to use. It consists of a library of words, organized by their grammatical roles, such as nouns, adjectives, verbs, adverbs, and which contains the various senses a word could possibly employ, as well as its various synonyms. It thus presents an interesting compound between a dictionary and a thesaurus, making it especially applicable to text analysis and artificial intelligence.

In addition to the simple definitions, WordNet presents a view of a semantic network between words. Example annotations for nouns are hypernyms (Y is an hypernym of X if X is a kind of Y, e.g. \emph{bird}\ is an hypernym of \emph{canary}) or hyponyms (the opposite relationship).

The database represents more than a hundred and fifty thousand words that have been extracted from published texts and manually annotated. It is thus an impressively large corpus that can present many applications.

On top of the WordNet database, another project called WordNet Domains has taken root, adding a new way of organizing the information found within the library. Taking as base the Dewey Index, the creating team at the Fondazione Bruno Kessler has assigned one or more domain to each synonymous set or sense a word can take, organizing the whole WordNet corpus in around 200 categories that represent human interests.
% section sources_of_input_data (end)

\section{Query Analysis} % (fold)
\label{sec:query_analysis}

In general, we consider Query Analysis to encompass the set of techniques that allow a program to translate an input question into a form that can be acted upon by an information retrieval program, be it a relational database or a complex question-answering system like SeCo.

In particular, the input can be presented in many forms. The first and simplest of these is to require a completely formal and structured syntax, such as the case of SQL. In this case, a parser for a context-free grammar can formally translate the input or outright reject it without any doubt to the result of the query. Of course, in that case, the burden is put upon the end user to enter a valid query as the analysis is strict. While this presents many advantages in the realms of consistency and reliability, it is often an impossible task a user to comply with, which makes it impractical for an end-user application.

The opposite to this previous representation is to allow a free form input to take place, accepting liberally and trying to make sense of it. This moves the burden from the user to the computer program who has to accept a dizzying range of possible inputs, with possible grammatical mistakes, strange constructions or unclear demands. Even then, there are many possible levels in which the query can be analyzed.

The most widely used model today is to take the input from the user as a simple bag of words, discarding the parts that present little differentiating power or that are not understood. This is a simple approach that works well when a simple match between the keywords and the results can made, for example in the field of document retrieval, where the presence of the words in a document is a good indicator of its relevance and should award its presence in the results.

While this is an interesting approach that warrants good results, it is limited by its simplicity, as it does not allow to understand the full expression of the language. Links between words and parts of the sentence are ignored, and the structure of the sentence, which in many cases can be quite revealing, is flattened into a single level.

While, in the end, some flattening must take place to reduce the question to a simpler representation, advanced techniques allow to make this procedure in a more intelligent fashion, and make use of the general category of tools that can be grouped under the term \emph{Natural Language Processing}.

\subsection{Natural Language Processing} % (fold)
\label{sub:natural_language_processing}

% subsection natural_language_processing (end)

\section{Parse Trees} % (fold)
\label{sec:parse_trees}

% section parse_trees (end)

% section query_analysis (end)

\section{Use of Query Analysis within SeCo} % (fold)
\label{sec:use_of_query_analysis_within_seco}

% section use_of_query_analysis_within_seco (end)

\section{Originality} % (fold)
\label{sec:originality}

% section originality (end)

% chapter background (end)
