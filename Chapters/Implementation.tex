%!TEX root = /Users/jp/Thesis/Thesis.tex
\chapter{Implementation} % (fold)
\label{cha:implementation}

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/architecture}
  \end{center}
  \caption{The High-level Architecture}\label{fig:architecture}
\end{figure}

\section{Tools} % (fold)
\label{sec:tools}

While some tools have been created expressly for the context of this research, many off-the-shelf technologies, tools and libraries were used in order to enhance the result and facilitate the creation of the final result. It is important to note that all these tools are open and freely available to the public.

\paragraph{JavaScript Object Notation} % (fold)
\label{par:javascript_object_notation}

The Javascript Object Notation or JSON is a text-based data interchange format similar to XML. It is based on the Javascript syntax for describing data structures. It supports a variety of data structures, the most commonly used in high-level languages. These include the basic types of strings and numbers, as well as the compound data types of lists (or arrays) and dictionaries (also called maps). Here is an example of a JSON data structure.

\begin{verbatim}
  {
    "string": "a string",
    "number": 10,
    "list": ["a", 3, ["12", 12]],
    "object": { "property": 12 }
  }
\end{verbatim}

It was chosen over other data exchange formats such as XML for its simplicity and readability. Of primordial importance was the ease in which it maps to the data types provided by most languages, which make it much more natural to convert back and forth. It also enjoys a good support over a multitude of languages and frameworks, libraries having been implemented in every high-level language available.

% paragraph javascript_object_notation (end)

\paragraph{Stanford Parser} % (fold)
\label{par:stanford_parser}

The Stanford Parser is a Natural Language Processing suite of tools and libraries that can be used in various tasks related to natural language analysis. In the context of this research, it is used for its parsing abilities. It is based on a probabilistic model, where, aided by trained that, it returns the \emph{n}\ most probably parsings of the tree.

It is implemented as a Java library accompanied by a dictionary file that is used as training data. While we use only the English corpus, others are available in German and Chinese.

% paragraph stanford_parser (end)

\paragraph{Kestrel} % (fold)
\label{par:kestrel}

To distribute work tasks amongst the worker, and to send it from the server where the manager of the workers can reach it, we use a queueing service known as Kestrel. While it is quite new, it has proven its worth through use at Twitter Inc.\footnote{See Twitter \amper\ kestrel}, where it powers a lot of the hugely-popular communication service. The particularity of this service is that it complies with the Memcached protocol. Memcached is the most widely-used service to store transient data and is used as a cache to avoid repeating costly operations. While it changes the semantics of this protocol, the fact that Kestrel respects the simple \emph{get} and \emph{set} contract of Memcached allows the use of a great number of libraries that, while originally made for Memcached itself, can now be used transparently to send tasks to the Kestrel server.

Its basic semantics are that a \emph{set} operation associates a key, with a queue, and the payload given within the operation will be added to the end of that named queue. The \emph{get}\ operation instead takes the first element from that same named queue, or returns a special message if no element can be found.

Kestrel itself is implemented as a daemon in Scala, a high-level language that takes most of its inspiration from Java and is in fact compiled to Java bytecode, allowing to run seamlessly in the Java Virtual Machine.

% paragraph kestrel (end)

\paragraph{CouchDB} % (fold)
\label{par:couchdb}

Used a data storage mechanism, CouchDB\footnote{See http://couchdb.apache.org/}\ is an Apache Foundation project for a document-based database server written in Erlang, a highly-efficient language for concurrent and distributed applications. It diverges itself from the model of relational databases in many ways, and offers a very different performance profile. CouchDB, as his description says, stores free-form documents instead of records as can be seen in a regular relational database. Its schemas are flexible, and the elements can change from one document to another in a same database. This makes many senses in many different applications, such as ones where schemas are highly likely to change over time, or in situations where the rows are very sparse, that is, many fields are present but only few are actually used in a single document.

It uses JSON as its native data format, making it very flexible in terms of what data types can be stored. It also supports computed views, which replace indices, and which are created in JavaScript by the user. These views follow the Map/Reduce paradigm, where a first function is tasked with going over every document, emitting a key/value pair which can be any given JSON element. The second function, then sorts and groups elements by their keys, and transforms and reduces the array of values associated with that key into a singular atomic element. The contract is that the computation of one element is totally independent from the computation of any other, allowing the system to distribute the work, cache it aggressively and reorder it as needed in order to improve the performance.

It also supports keeping multiple revisions of a single document, allowing the user to require a particular version. It also gives the opportunity to offer optimistic conflict resolution for updates, where, during an update operation, the sender is required to state which version its change is based on. If that version corresponds to the currently most up-to-date, the update is made without any trouble. In the other case, if another user had already updated the same document, an error message is sent to the user who is then given the opportunity to rebase himself on the latest version.

Other interesting features of CouchDB are its core support for master-master replication, where two nodes can be synchronized and where both can still act as master, unlike the normal model master-slave model where slaves are only used for read operations while the master is the unique point of update.

CouchDB was chosen within the context of this project with the idea that the schema was most likely to change greatly over the progress of the research, and that the objects that would needed to be stored would not fit a relational database very well. Examples of this include the result from the parse tree, which take the form of a very deep tree, or the set of objects for a single split domain.

% paragraph couchdb (end)

\paragraph{Ruby} % (fold)
\label{par:ruby}

Ruby is a high-level programming language known amongst aficionados as being highly dynamic and flexible in regards to its syntax. While its implementation is relatively slower than other languages', it has become famous for allowing the creation of DSL, Domain-Specific Languages, where the host language itself is adapted in order to create a more natural syntax adapted to the task at hand. In particular, it has become famous for its use in the Web domain, where it now sports a host of libraries adapted to quickly and efficiently creating Web Applications.

It is a pure object-oriented language, where every method or function is actually activated by sending a message to the desired instance. Every element in the code can be considered as objects, even literal string and numbers. It follows in this the tradition of the Smalltalk chain of languages. It also allows the re-opening and modification of already-defined classes, even the ones that make part of the core and standard library.

% paragraph ruby (end)

\paragraph{Sinatra} % (fold)
\label{par:sinatra}

Sinatra\footnote{See http://www.sinatrarb.com}\ is a next-generation Web Framework built in the Ruby language. its particularities are its extreme simplicity while still keeping most of the power of other frameworks, a simplicity that also offers a great flexibility. It is still firmly rooted within the realm of Model-View-Controller frameworks. While it doesn't specify anything about the models, it organizes the code around controllers who match some specified URL. Any given operation can be made within those blocks of code, and the only contract is that they are expected to return a string of characters that will be sent to the user. This string can be generated directly, or, as it is preferable, created from the rendering of a specified template that will abstract away the view part.

Sinatra itself can be run on a number of application servers, which range from small, focused ones such as Thin \footnote{see code.macournoyer.com/thin}\ or general Web servers such as Apache.

% paragraph sinatra (end)

\paragraph{HTTParty} % (fold)
\label{par:httparty}

While fetching and parsing data from an external Web service can be made using low-level library, HTTParty makes it much easier. One simply has to specify the URL of the service as well as optional parameters, such as the developer key in the case of Yahoo! Answers, and HTTParty will take care of fetching the data and translating it in a native format of its host language, Ruby.

% paragraph httparty (end)

\paragraph{Scala} % (fold)
\label{par:scala}

Scala is a high-level language closely based on Java, but taking on the ideas of the latest generation of functional languages, such as Haskell or ML, and others from dynamic languages like Ruby. It was created in Switzerland in a project led by Martin Odersky, a lead designer on the Java language itself. Scala offers quasi-total compatibility with Java, being able to import and export libraries compiled in any language running on the Java Virtual Machine. In addition, while it sports a Java-like syntax, it supports type inferencing, allowing users to skip explicitly defining the type of each variable. In addition, it supports high-level functions, pattern matching and an evolution of interfaces and abstract classes called traits, inspired by Ruby mixins.

Among its remarkable features can be found a library that offer a new perspective on concurrent systems, called actors. This feature, taken from languages such as Erlang and Smalltalk, allows one developer to conceptualize systems as a series of independent processes called actors, where these can communicate through the use of referentially-transparent messages.

It was chosen first and foremost because it offers access to the wide libary of Java applications. It was also then chosen over Java itself because it is more suited to explorative programming, where one does not know exactly the shape the result will take, as was the case at the beginning of this project. Furthermore, its actors paradigm can easily be used to model the concept of worker tasks receiving orders as messages from an external service.

% paragraph scala (end)

% section tools (end)

\section{Architecture} % (fold)
\label{sec:architecture}

The general framework that powers this research is linked around the two top-level tasks, that is the creation of the corpus, and the use of this corpus within the context of query analysis. In this fashion, the tool created to support these tasks must be flexible enough to accommodate both diverging needs that are centered around a single set of data.

The center of it is thus the Web front-end that powers the creation of the corpus as well as provides a view into the results of the algorithms employed to analyze the queries and extract the domains. This front-end has a first communication with the outside through the use of the Yahoo! Answers Web service, where, on request of the user browsing the site, it requests additional entries from the Web service and shows the new entries to the server. To store and retrieve these entries, it communicates with the document database.

Since the processing required to analyze the queries is non-negligible, both in terms of CPU usage as well as memory, it would be difficult to require the extraction and the analysis to be done in real time in the same environment as the database and the Web front-end. Thus, a mechanism to offload the work into another computer has been devised. Based on a standard and simple architecture for background workers, the Web front-end, or the user through the command line, can post work items on a queueing server where it will be picked up by one of the clients, the first one who is available to compute. The client then processes the task with the given input parameters, and stores the results back into the document database, which the Web front-end can then present to the user.

% section architecture (end)

\section{Sift - Data Extraction \amper\ Rating} % (fold)
\label{sec:sift_data_extraction_and_rating}

The Web front-end and the tool used to extract data from the Yahoo! Answers Web Service, nicknamed Sift, is a Web Application based on the Sinatra framework, and thus has been written in the Ruby programming language. In addition to the main Sinatra library for web application development, it imports the Ruby libraries used to interface with the CouchDB server, the Kestrel queue service as well as the Yahoo! Answers Web Service. The application is divided into three principal parts: the Models, the Controllers and the Views.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/siftdatamodels}
  \end{center}
  \caption{Sift - Data Models}\label{fig:sift_data_models}
\end{figure}

There are two models in Sift, as one can see in Figure~\ref{fig:sift_data_models}. The first one corresponds to the entries that are stored in the document database. These entries contain the input data, either taken from Yahoo! Answers or entered manually. They also contain the rating given to the entry itself, as well as the evaluations of the individual strategies employed on the element. Creating or updating an entry within the confines of this tool automatically make it post a message to the queue server, if it is available, where the task can be picked up by the background worker.

The second one corresponds to the result of a query made to the Yahoo! Answers website, before it is inserted in the document database where it becomes an Entry. It contains the fields of the content provided by the Yahoo! API, which, of interest to us, are the category identifier, the question identifier, the question title as well as the body.

The controllers in Sift correspond to a series of functions that are called when a pattern matching a URL is matched. The most important method in this case is the one for the index page, where most of the work is done. Its basic task is to prepare the list of entries that are to be shown to the user. In order to do this, it takes a list of parameters fed by the user. These can include the number of items to show, the page to show, the ordering, a filtering by the rating given to an entry or the lack of it. Before explaining how the interface is then created from this list of entries, let us review the other methods available. The interface allows for the manual creation of an entry, for which a method is thus available. In a similar fashion, methods are exposed for the user to rate an entry, either as a whole, in the case of the extraction of the corpus, or for an specific feature, useful in the matter of the evaluation of the strategies.

The last part is the view module, where a template is processed, taking as input the different variables prepared by the controller. In the case of the index page, where the list of entries is shown, the output HTML contains the list of all the entries, with each entry containing the results of the processing done if there was any, although this part is hidden at first.

The interface is heavily inspired by Google Mail\footnote{See http://mail.google.com/}\ where a list of summaries is shown and can be navigated through, either using the mouse or the keyboard. Elements can be given a rating by clicking the corresponding star to the right of it. It is also possible to change the rating on more than one element at once by selecting them first and then using the drop-down action menu or the keyboard shortcuts to give it a new rating.

Clicking a single entry will show the details for it if it has been processed by the background worker system, in which case, it is possible to see the alternative strategies and rate them individually.

This interface was created using the jQuery toolkit for JavaScript, which allows a high-level view of the web page, allowing to query and manipulate elements, as well as make asynchronous calls to the servers, in a stable and consistent manner against all the browsers.

% section sift_data_extraction_and_rating (end)

\section{Bee - Distributed Background Processing} % (fold)
\label{sec:bee_distributed_background_processing}

\begin{table}
  \caption{Execution Profile Data Model}
  \label{tab:execution_profile}
  
  \begin{center}
    \begin{tabular}{c | c}
      Element & Type\\
      \hline
      Errors & List of Strings\\
      Result & JSON value\\
      Duration & Milliseconds\\
      Version & String\\
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/beeuml}
  \end{center}
  \caption{Bee - Class Diagram}\label{fig:beeuml}
\end{figure}



\begin{table}
  \caption{The task interface}
  \label{tab:task_interface}
  
  \begin{center}
    \begin{tabular}{c | l}
      Method & Description\\
      \hline
      setup(configuration) & Optionally implemented, provides a mean for the task to access its configuration, and use it to set itself up, for example by loading data dictionaries. This is only done once when the task is first loaded, so it is a good opportunity to cache things\\
      identifier: string & This method returns the unique identifier of the task within the chain\\
      run(inputParams): json & This is the core method of the task. It takes as input a Map of the input given to it from the queue as well as the results of the previous tasks in the chain, arranged according to their identifier. The output of the function should be a json value. Any exception thrown by this task will interrupt the chain and will be stored in the errors fields in the database.\\
      version: string & Allows a task to report its version, forcing the re-computation of all the elements in a chain starting from this task.\\
    \end{tabular}
  \end{center}
\end{table}
% section bee_distributed_background_processing (end)


\section{Data Gathering \amper\ Analysis} % (fold)
\label{sec:data_gathering_and_analysis}

In order to grasp the intricacies of the problem of parsing natural language queries in our particular domain, we tried to look for a source of a large number of already-made queries that correspond to our criteria, that is that they span multiple domains and that they are expressed naturally. We found Yahoo! Answers to be a good source for such kind of data, albeit with a very low signal to noise ratio. That is, a first step on the way to obtaining a valuable corpus would be to extract and filter the data from this site. Thankfully, Yahoo! Answers provide an Application Programming Interface that allows us to obtain questions in large quantities in a XML or JSON format. We thus created a Web Tool, code-named Siphon, that extracts the questions from Yahoo! Answers and present them to the user where he or she can decide whether to accept or reject each entry.

\begin{huge}
  Insert Screenshot
\end{huge}

This tool presents a simple list view where, by default, the yet unfiltered entries are shown. Through the use of keyboard shortcuts or the provided links, the user can circle through the entries, accepting or rejecting them. The entries are stored permanently, whether accepted or not, and it is then possible to change mode and go look at the previously accepted or rejected entries, before coming back to filter new ones. The tool will load new entries from Yahoo! Answers as the number of unfiltered entries get low, insuring a steady amount of entries to filter.

Another interesting functionality of this tool is the ability to inspect individual entries, giving a view of how they would be parsed using both the shallow parser and the Stanford parser, which shall be described further. Parsing a large corpus such as this one is not a trivial matter, and so, a second tool was developed to offload and distribute the parsing tasks to diverse computers. This tool, called Bee, uses a simple queue service as a central dispatching mechanism and connects to a document database to store the results. Its concept is centered around tasks, defined by the developer, which are run by workers waiting for messages on a pre-defined queue. In answer to these parameterized messages, the worker will launch the task and store its result on the database. On the other side, we have the Siphon tool posting messages to this queue as new entries are created, and using the results from the document database to show to the user.

The queue used is Kestrel, which is a simple open-source queue implementation in Scala reusing the now common Memcache protocol, and which has proved its stability and performance at Twitter Inc. where it was developed. As we could expect the schema of the documents to change often as new tasks are added to the service, a schema-less database was chosen. Given the breadth of technology used, the Apache CouchDB project was used as it internally keeps data in the JSON format which has a wide support across different languages. The data structures available in JSON, like objects and arrays, were an essential asset for the transcoding of the parse trees to a common format. The Bee software itself is also written in Scala, providing an easy object-oriented concurrency model based on the actors concept.

In the spirit of open source and given the strong use of it during the development of this project, the two tools described here are made available as free and open source software and can be found at www.github.com/jpbougie.

% section data_gathering_and_analysis (end)

\section{Parsers} % (fold)
\label{sec:parsers}

The first phase of the query analysis was to extract the grammatical structure of the question asked by the user. The existing approaches that are currently widely used are based either on formal grammars, or on keyword extraction.

The first approach is to require the user to input a very structured and often unnatural query. The best example that can be given is that of SQL, the Structured Query Language, that specifies a declarative programming language used by the client as the only mean to access the data. Queries formally define the kind of data that is to be retrieved, the 


 In order to do that, it was decided to use a natural language parser, that can turn a simple sentence into an annotated series of tokens organized in a hierarchical manner according to the grammatical structures and features of the sentence. Two such tools were chosen to be evaluated, the Stanford Natural Language Parser and the University of Illinois at Urbana-Champaign's Shallow Parser. 

While they both use different techniques and thus produce different results for the same input, they have great similarities. Both are based on a statistical and probabilistic analysis that will discern the most likely structure from the tokens in the sentence, assigning each one a role in the sentence as well a grouping the structures together.

\subsection{Stanford Parser} % (fold)
\label{sub:stanford_parser}
The Stanford Parser is a Java library created at Stanford University that is able to turn natural language input into structured trees annotated with the grammatical roles of each part of the sentence and the structure that links those tokens together. Here is an example of what the stanford parser would output for a simple input:
% subsection stanford_parser (end)
% section parsers (end)

% chapter implementation (end)