%!TEX root = /Users/jp/Thesis/Thesis.tex
\chapter{Implementation} % (fold)
\label{cha:implementation}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\linewidth]{images/architecture}
  \end{center}
  \caption{The High-level Architecture}\label{fig:architecture}
\end{figure}

\section{Tools} % (fold)
\label{sec:tools}

While some tools have been created expressly for the context of this research, many off-the-shelf technologies, tools and libraries were used in order to enhance the result and facilitate the creation of the final result. It is important to note that all these tools are open and freely available to the public.

\paragraph{JavaScript Object Notation} % (fold)
\label{par:javascript_object_notation}

The Javascript Object Notation or JSON is a text-based data interchange format similar to XML. It is based on the Javascript syntax for describing data structures. It supports a variety of data structures, the most commonly used in high-level languages. These include the basic types of strings and numbers, as well as the compound data types of lists (or arrays) and dictionaries (also called maps). Here is an example of a JSON data structure.

\begin{verbatim}
  {
    "string": "a string",
    "number": 10,
    "list": ["a", 3, ["12", 12]],
    "object": { "property": 12 }
  }
\end{verbatim}

It was chosen over other data exchange formats such as XML for its simplicity and readability. Of primordial importance was the ease in which it maps to the data types provided by most languages, which make it much more natural to convert back and forth. It also enjoys a good support over a multitude of languages and frameworks, libraries having been implemented in every high-level language available.

% paragraph javascript_object_notation (end)

\paragraph{Stanford Parser} % (fold)
\label{par:stanford_parser}

The Stanford Parser is a Natural Language Processing suite of tools and libraries that can be used in various tasks related to natural language analysis. In the context of this research, it is used for its parsing abilities. It is based on a probabilistic model, where, aided by trained that, it returns the \emph{n}\ most probably parsings of the tree.

It is implemented as a Java library accompanied by a dictionary file that is used as training data. While we use only the English corpus, others are available in German and Chinese.

% paragraph stanford_parser (end)

\paragraph{Kestrel} % (fold)
\label{par:kestrel}

To distribute work tasks amongst the worker, and to send it from the server where the manager of the workers can reach it, we use a queueing service known as Kestrel. While it is quite new, it has proven its worth through use at Twitter Inc.\footnote{See Twitter \amper\ kestrel}, where it powers a lot of the hugely-popular communication service. The particularity of this service is that it complies with the Memcached protocol. Memcached is the most widely-used service to store transient data and is used as a cache to avoid repeating costly operations. While it changes the semantics of this protocol, the fact that Kestrel respects the simple \emph{get} and \emph{set} contract of Memcached allows the use of a great number of libraries that, while originally made for Memcached itself, can now be used transparently to send tasks to the Kestrel server.

Its basic semantics are that a \emph{set} operation associates a key, with a queue, and the payload given within the operation will be added to the end of that named queue. The \emph{get}\ operation instead takes the first element from that same named queue, or returns a special message if no element can be found.

Kestrel itself is implemented as a daemon in Scala, a high-level language that takes most of its inspiration from Java and is in fact compiled to Java bytecode, allowing to run seamlessly in the Java Virtual Machine.

% paragraph kestrel (end)

\paragraph{CouchDB} % (fold)
\label{par:couchdb}

Used a data storage mechanism, CouchDB\footnote{See http://couchdb.apache.org/}\ is an Apache Foundation project for a document-based database server written in Erlang, a highly-efficient language for concurrent and distributed applications. It diverges itself from the model of relational databases in many ways, and offers a very different performance profile. CouchDB, as his description says, stores free-form documents instead of records as can be seen in a regular relational database. Its schemas are flexible, and the elements can change from one document to another in a same database. This makes many senses in many different applications, such as ones where schemas are highly likely to change over time, or in situations where the rows are very sparse, that is, many fields are present but only few are actually used in a single document.

It uses JSON as its native data format, making it very flexible in terms of what data types can be stored. It also supports computed views, which replace indices, and which are created in JavaScript by the user. These views follow the Map/Reduce paradigm, where a first function is tasked with going over every document, emitting a key/value pair which can be any given JSON element. The second function, then sorts and groups elements by their keys, and transforms and reduces the array of values associated with that key into a singular atomic element. The contract is that the computation of one element is totally independent from the computation of any other, allowing the system to distribute the work, cache it aggressively and reorder it as needed in order to improve the performance.

It also supports keeping multiple revisions of a single document, allowing the user to require a particular version. It also gives the opportunity to offer optimistic conflict resolution for updates, where, during an update operation, the sender is required to state which version its change is based on. If that version corresponds to the currently most up-to-date, the update is made without any trouble. In the other case, if another user had already updated the same document, an error message is sent to the user who is then given the opportunity to rebase himself on the latest version.

Other interesting features of CouchDB are its core support for master-master replication, where two nodes can be synchronized and where both can still act as master, unlike the normal model master-slave model where slaves are only used for read operations while the master is the unique point of update.

CouchDB was chosen within the context of this project with the idea that the schema was most likely to change greatly over the progress of the research, and that the objects that would needed to be stored would not fit a relational database very well. Examples of this include the result from the parse tree, which take the form of a very deep tree, or the set of objects for a single split domain.

% paragraph couchdb (end)

\paragraph{Ruby} % (fold)
\label{par:ruby}

Ruby is a high-level programming language known amongst aficionados as being highly dynamic and flexible in regards to its syntax. While its implementation is relatively slower than other languages', it has become famous for allowing the creation of DSL, Domain-Specific Languages, where the host language itself is adapted in order to create a more natural syntax adapted to the task at hand. In particular, it has become famous for its use in the Web domain, where it now sports a host of libraries adapted to quickly and efficiently creating Web Applications.

It is a pure object-oriented language, where every method or function is actually activated by sending a message to the desired instance. Every element in the code can be considered as objects, even literal string and numbers. It follows in this the tradition of the Smalltalk chain of languages. It also allows the re-opening and modification of already-defined classes, even the ones that make part of the core and standard library.

% paragraph ruby (end)

\paragraph{Sinatra} % (fold)
\label{par:sinatra}

Sinatra\footnote{See http://www.sinatrarb.com}\ is a next-generation Web Framework built in the Ruby language. its particularities are its extreme simplicity while still keeping most of the power of other frameworks, a simplicity that also offers a great flexibility. It is still firmly rooted within the realm of Model-View-Controller frameworks. While it doesn't specify anything about the models, it organizes the code around controllers who match some specified URL. Any given operation can be made within those blocks of code, and the only contract is that they are expected to return a string of characters that will be sent to the user. This string can be generated directly, or, as it is preferable, created from the rendering of a specified template that will abstract away the view part.

Sinatra itself can be run on a number of application servers, which range from small, focused ones such as Thin \footnote{see code.macournoyer.com/thin}\ or general Web servers such as Apache.

% paragraph sinatra (end)

\paragraph{HTTParty} % (fold)
\label{par:httparty}

While fetching and parsing data from an external Web service can be made using low-level library, HTTParty makes it much easier. One simply has to specify the URL of the service as well as optional parameters, such as the developer key in the case of Yahoo! Answers, and HTTParty will take care of fetching the data and translating it in a native format of its host language, Ruby.

% paragraph httparty (end)

\paragraph{Scala} % (fold)
\label{par:scala}

Scala is a high-level language closely based on Java, but taking on the ideas of the latest generation of functional languages, such as Haskell or ML, and others from dynamic languages like Ruby. It was created in Switzerland in a project led by Martin Odersky, a lead designer on the Java language itself. Scala offers quasi-total compatibility with Java, being able to import and export libraries compiled in any language running on the Java Virtual Machine. In addition, while it sports a Java-like syntax, it supports type inferencing, allowing users to skip explicitly defining the type of each variable. In addition, it supports high-level functions, pattern matching and an evolution of interfaces and abstract classes called traits, inspired by Ruby mixins.

Among its remarkable features can be found a library that offer a new perspective on concurrent systems, called actors. This feature, taken from languages such as Erlang and Smalltalk, allows one developer to conceptualize systems as a series of independent processes called actors, where these can communicate through the use of referentially-transparent messages. Actors are implemented using a mailbox, in fact a queue where messages are stored. The actor can then define its \emph{act}\ method to handle these messages, often using pattern matching to dispatch in view of the type of the message which can be arbitrary.

An example of a simple actor which receives elements from the Message class and prints their content to the console follows.

\begin{verbatim}
  case class Message(payload: String)
  class EchoActor extends Actor {
    def act = {
      loop {
        react {
          case Message(content) => Console.println("Received a message" + content)
        }
      }
    }
  }
\end{verbatim}

It was chosen first and foremost because it offers access to the wide libary of Java applications. It was also then chosen over Java itself because it is more suited to explorative programming, where one does not know exactly the shape the result will take, as was the case at the beginning of this project. Furthermore, its actors paradigm can easily be used to model the concept of worker tasks receiving orders as messages from an external service.

% paragraph scala (end)

% section tools (end)

\section{Architecture} % (fold)
\label{sec:architecture}

The general framework that powers this research is linked around the two top-level tasks, that is the creation of the corpus, and the use of this corpus within the context of query analysis. In this fashion, the tool created to support these tasks must be flexible enough to accommodate both diverging needs that are centered around a single set of data.

The center of it is thus the Web front-end that powers the creation of the corpus as well as provides a view into the results of the algorithms employed to analyze the queries and extract the domains. This front-end has a first communication with the outside through the use of the Yahoo! Answers Web service, where, on request of the user browsing the site, it requests additional entries from the Web service and shows the new entries to the server. To store and retrieve these entries, it communicates with the document database.

Since the processing required to analyze the queries is non-negligible, both in terms of CPU usage as well as memory, it would be difficult to require the extraction and the analysis to be done in real time in the same environment as the database and the Web front-end. Thus, a mechanism to offload the work into another computer has been devised. Based on a standard and simple architecture for background workers, the Web front-end, or the user through the command line, can post work items on a queueing server where it will be picked up by one of the clients, the first one who is available to compute. The client then processes the task with the given input parameters, and stores the results back into the document database, which the Web front-end can then present to the user.

% section architecture (end)

\section{Sift - Data Extraction \amper\ Rating} % (fold)
\label{sec:sift_data_extraction_and_rating}

The Web front-end and the tool used to extract data from the Yahoo! Answers Web Service, nicknamed Sift, is a Web Application based on the Sinatra framework, and thus has been written in the Ruby programming language. In addition to the main Sinatra library for web application development, it imports the Ruby libraries used to interface with the CouchDB server, the Kestrel queue service as well as the Yahoo! Answers Web Service. The application is divided into three principal parts: the Models, the Controllers and the Views.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/siftdatamodels}
  \end{center}
  \caption{Sift - Data Models}\label{fig:sift_data_models}
\end{figure}

There are two models in Sift, as one can see in Figure~\ref{fig:sift_data_models}. The first one corresponds to the entries that are stored in the document database. These entries contain the input data, either taken from Yahoo! Answers or entered manually. They also contain the rating given to the entry itself, as well as the evaluations of the individual strategies employed on the element. Creating or updating an entry within the confines of this tool automatically make it post a message to the queue server, if it is available, where the task can be picked up by the background worker.

The second one corresponds to the result of a query made to the Yahoo! Answers website, before it is inserted in the document database where it becomes an Entry. It contains the fields of the content provided by the Yahoo! API, which, of interest to us, are the category identifier, the question identifier, the question title as well as the body.

The controllers in Sift correspond to a series of functions that are called when a pattern matching a URL is matched. The most important method in this case is the one for the index page, where most of the work is done. Its basic task is to prepare the list of entries that are to be shown to the user. In order to do this, it takes a list of parameters fed by the user. These can include the number of items to show, the page to show, the ordering, a filtering by the rating given to an entry or the lack of it. Before explaining how the interface is then created from this list of entries, let us review the other methods available. The interface allows for the manual creation of an entry, for which a method is thus available. In a similar fashion, methods are exposed for the user to rate an entry, either as a whole, in the case of the extraction of the corpus, or for an specific feature, useful in the matter of the evaluation of the strategies.

The last part is the view module, where a template is processed, taking as input the different variables prepared by the controller. In the case of the index page, where the list of entries is shown, the output HTML contains the list of all the entries, with each entry containing the results of the processing done if there was any, although this part is hidden at first.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/siftlist}
  \end{center}
  \caption{Sift - List View}\label{fig:sift_list}
\end{figure}


The interface is heavily inspired by Google Mail\footnote{See http://mail.google.com/}\ where a list of summaries is shown and can be navigated through, either using the mouse or the keyboard. Elements can be given a rating by clicking the corresponding star to the right of it. It is also possible to change the rating on more than one element at once by selecting them first and then using the drop-down action menu or the keyboard shortcuts to give it a new rating.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/siftentry}
  \end{center}
  \caption{Sift - Single Entry View}\label{fig:sift_entry}
\end{figure}


As can be seen in Figure~\ref{fig:sift_entry}, clicking a single entry will show the details for it if it has been processed by the background worker system, in which case, it is possible to see the alternative strategies and rate them individually.

This interface was created using the jQuery toolkit for JavaScript, which allows a high-level view of the web page, allowing to query and manipulate elements, as well as make asynchronous calls to the servers, in a stable and consistent manner against all the browsers.

% section sift_data_extraction_and_rating (end)

\section{Bee - Distributed Background Processing} % (fold)
\label{sec:bee_distributed_background_processing}

The other major part of the tool chain created expressly for the subject of this research is the framework for background worker that is used to test query analysis and domain extraction strategies. It was decided to create such a system in order to be able to allow asynchronous processing, as well as being able to offload the main server. This system, nicknamed Bee, was created in Scala and uses libraries to convert to and from JSON, to access CouchDb as well as the Kestrel queue service. Refer to the Figure~\ref{fig:beeuml} for an overview of the classes that compose this framework.

\paragraph{Tasks} % (fold)
\label{par:tasks}

The central concept in Bee is that of a task. A task is a single unit of work that is coded by the user of the library.  Tasks themselves are centered around one function, run, which takes as parameters the original input fed from the queue, as well as the results of the previous tasks. In turn, the tasks return the results of their calculations in the JSON format. These tasks are organized in chains, where each chain is given a unique identifier. It is the ultimate result of the chain that is stored back into the document database. Table~\ref{tab:task_interface} presents an overview of the methods of the Task interface.

% paragraph tasks (end)

\paragraph{Execution Profile} % (fold)
\label{par:execution_profile}

The result of the execution of a single task in wrapped in a class called Execution Profile, which contains, in addition to the raw result, a list of errors or exceptions the task has encountered, the version of the task that was executed and, finally, the time it took for the task to be executed. You can refer to the table~\ref{tab:execution_profile} for a summary of the fields of the execution profile.

% paragraph execution_profile (end)

\paragraph{Workers} % (fold)
\label{par:workers}

Spawned at the number of one by defined chain, the workers take care of interacting with the queue service and overview the execution of the tasks in their chain. At the initialization, they are given a unique identifier, the name of the chain as defined in the configuration. They then setup every task before opening a connection on the queue, waiting for messages in the queue named by their identifier. Once a message is received, they first check into the database to see if this particular instance of the chain had already been executed. If that is the case, the existing data is fetched and parsed. The tasks will be skipped as long as no errors have been encountered and no version of a task has changed.

Once all the remaining tasks have been executed, the worker asks the database actor to store the updated document before resuming listening on the queue.

% paragraph workers (end)

\paragraph{Splitting Tasks} % (fold)
\label{par:splitting_tasks}

Splitting Tasks are a specialization of the generic task, in which the parsing and serialization is already taken care of. The user only has to define a function, split, that will transform the input from a tree-based representation to a series of parts.

Each part is an instance of a class composed of two fields, the first being the phrase, or sub-sentence that is considered as that part. The second element is the list of the objects retrieved from that phrase, where each object is a couple made of the object itself as well as its part of speech (e.g. \emph{verb}, \emph{noun}\ or \emph{adjective}).

% paragraph splitting_tasks (end)

\paragraph{Domain Extraction Tasks} % (fold)
\label{par:domain_extraction_tasks}

Domain extraction tasks are another kind of specialized task. They obviously correspond to the third phase of the query analysis process, where the different parts of a sentence are analyzed in order to obtain a series of domains that will eventually be mapped to different query services.

Implementations of this interface once again have one single method to implement. This method, named extract, takes the list of parts obtained from the previous operation, and have to return, for each part of the sentence, a list of possible domains. Note that while there must be a list for every single part, that list can be empty if nothing of importance is found or if a word is not recognized.

% paragraph domain_extraction_tasks (end)


\paragraph{Hive - database access} % (fold)
\label{par:hive_database_access}

The class in charge of the access to the database, nicknamed Hive, responds to two simple message. The first one is a request to fetch the result of the execution of a chain from the database, given the name of the chain and the identifier of the original input, and replies to sender of the request with the resulting element. The second one is the opposite operation, that is, it takes the result of the chain in addition to its identifier and the identifier of the input, and stores it back into the database, possibly merging it with the existing data or the result of the execution of other chains.

The hive was designed as an independent actor to allow the workers to quickly send a request and then move on to the execution of the next task. Given that the requests are put into a single mailbox implemented as a queue, it also allows the system to avoid problems of concurrent update if two workers executed different task chains on the same input element.

% paragraph hive_database_access (end)

\paragraph{Configuration} % (fold)
\label{par:configuration}

The Bee Framework can be configured using a file of the following format, which is given to the program at its launch. The configuration file is divided in four major sections.

The first section allows the user to configure the connection to the queueing system, with the host and the port taken as parameters. The additional parameter, \emph{wait}, specifies an extension given by the kestrel protocol, where an attempted read on the queue can block it for a period of times, thus avoiding the overhead of constant polling.

The second section works in a similar manner, but specifies the connection to the document database, allowing the parameters of the host, the port, as well as the name of the database.

The most important configuration section is that of the chains, where a series of task chains can be defined. Each chain is made of a single element, \emph{chain}, which define a list of chains to be executed. Each atom in that list must then be defined, mandatorily giving the fully-qualified name of the class which contain the task to be executed. Optionally, additional parameters can be given, these will be passed to the task during the setup phase.

Finally, the logging options can be adjusted in the final section, allowing to change the verbosity of the output and where it is redirected.

\begin{verbatim}
  <queue>
    port = 22133
    host = "jpbougie.net"
    wait = 10 #time to wait for a message in the queue
  </queue>

  <couchdb>
    host = "couch.jpbougie.net"
    port = 80
    database = "sift"
  </couchdb>

  <chains>
    <stanford>
      <parser>
        class = "net.jpbougie.seco.StanfordParser"
      </parser>

      <flsplit>
        class = "net.jpbougie.seco.FirstLevelSplit"
      </flsplit>

      <clausesplit>
        class = "net.jpbougie.seco.ClauseSplit"
      </clausesplit>

      <fldomains>
        class = "net.jpbougie.seco.WordnetDomains"
        apply_to = "firstlevelsplit"
        domains_file = "/Users/jp/Projects/Thesis/wn-domains-3.2/wn-domains-3.2-20070223"
        wordnet_dir = "/Users/jp/Projects/Thesis/WordNet-2.0/dict"
      </fldomains>

      <cldomains>
        class = "net.jpbougie.seco.WordnetDomains"
        apply_to = "clausesplit"
        domains_file = "/Users/jp/Projects/Thesis/wn-domains-3.2/wn-domains-3.2-20070223"
        wordnet_dir = "/Users/jp/Projects/Thesis/WordNet-2.0/dict"
      </cldomains>

      <stats>
        class = "net.jpbougie.seco.ParseStats"
      </stats>

      chain = ["parser", "flsplit", "clausesplit", "fldomains", "cldomains", "stats"]
    </stanford>

  </chains>

  <log>
    level = "info"
  </log>
<\end>

\end{verbatim}

% paragraph configuration (end)

\begin{table}
  \caption{Execution Profile Data Model}
  \label{tab:execution_profile}
  
  \begin{center}
    \begin{tabular}{c | c}
      Element & Type\\
      \hline
      Errors & List of Strings\\
      Result & JSON value\\
      Duration & Milliseconds\\
      Version & String\\
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/beeuml}
  \end{center}
  \caption{Bee - Class Diagram}\label{fig:beeuml}
\end{figure}



\begin{table}
  \caption{The task interface}
  \label{tab:task_interface}
  
  \begin{center}
    \begin{tabular}{c | p{10cm} }
      Method & Description\\
      \hline
      setup(configuration) & Optionally implemented, provides a mean for the task to access its configuration, and use it to set itself up, for example by loading data dictionaries. This is only done once when the task is first loaded, so it is a good opportunity to cache things\\
      identifier: string & This method returns the unique identifier of the task within the chain\\
      run(inputParams): json & This is the core method of the task. It takes as input a Map of the input given to it from the queue as well as the results of the previous tasks in the chain, arranged according to their identifier. The output of the function should be a json value. Any exception thrown by this task will interrupt the chain and will be stored in the errors fields in the database.\\
      version: string & Allows a task to report its version, forcing the re-computation of all the elements in a chain starting from this task.\\
    \end{tabular}
  \end{center}
\end{table}
% section bee_distributed_background_processing (end)

\section{Parsing Strategies} % (fold)
\label{sec:parsing_strategies}

Once the framework has been established and stabilized, it was possible to start implementing the various strategies under the form of tasks. The first of these were obviously the parsing strategies, which take as input the natural language sentence and render explicit the grammatical structure.

\subsection{Tree Representation} % (fold)
\label{sub:tree_representation}

The output of such a process will take the form of an arbitrarily-deep tree, where a leaf represents an atom, a word in the sentence, while an internal node will represent a grouping of these words in some structure, for example in a noun phrase or verb phrase. Each node is also annotated with an optional score given by the parser for that particular sub-structure.

% subsection tree_representation (end)

\subsection{Stanford Parser} % (fold)
\label{sub:stanford_parser_impl}

The first parser to be evaluated was the Stanford Natural Language Parser. Distributed as a Java library, it requires little code to use. One simply needs to load the parser with the chosen training data file, and then apply the parser to a sentence to get a resulting tree. This tree is then transformed in order to take it from the native tree representation of the Stanford Parser into a generic one that is to be used by the later tasks. Here is an example on how to load and apply the parser, in Scala.

\begin{verbatim}
  // dataFile points to the training data set on the hard drie
  // input contains the natural language sentence
  
  import edu.stanford.nlp.parsers._
  
  val parser = new LexicalizedParser(dataFile)
  val tree = parser.apply(input)
  
\end{verbatim}

% paragraph tree (end)

% subsection stanford_parser (end)

\subsection{Shallow Parser} % (fold)
\label{sub:shallow_parser}

The Shallow Parser, developed at the University of Illinois at Urbana-Champaign, takes a different approach to obtaining the final result. It works by using a series of different tools that will process the input into a more and more complex form, as you can refer in Figure~\ref{fig:shallow_parser_flow}. The first of these steps is to sanitize the input, make sure that every element is well tokenized; that is, every element in the sentence is spaced out, even the punctuation. It also produces some slight transformations and normalization operations.

The output of this first operation is then sent to a second program, which takes care of tagging each element of the sentence with its most probable part of speech, be it noun, verb, adjective or other. This is then finally sent to a server called the \emph{chunking server}, which takes the annotated input and groups, or chunks, elements into what it thinks are the primordial structure of the sentence.

The \emph{shallow}\ name thus comes from the fact that this grouping operation is only done at one level, which means that the output could be formally defined as a sequence of elements which can either be atoms, or sub-sequences of such atoms. This output, given as text by the server, is then parsed by the task and put into a tree representation, although it is only going to be one level deep.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics[width=\linewidth]{images/shallowparserflow}
  \end{center}
  \caption{Shallow Parser - Data Flow}\label{fig:shallow_parser_flow}
\end{figure}

% subsection shallow_parser (end)

% section parsing_strategies (end)

\section{Sentence Splitting Strategies} % (fold)
\label{sec:sentence_splitting_strategies}

\subsection{From Tree to Parts} % (fold)
\label{sub:from_tree_to_parts}

Once we have a tree that suitable to use for parsing, we can proceed to the division of that structure into many parts, with the expectation that each part will correspond to a single domain of query.

The data structures that form the contract of that operation are thus two-fold; a first one, seen previously, is the tree resulting from the parsing operation. In output instead, each part of the sentence is to be represented by an instance of the part class, which contains the extracted sub-sentence as well as, taken from that, the objects that are considered important to the definition of the domain.

% subsection from_tree_to_parts (end)

\subsection{First-Level Split} % (fold)
\label{sub:first_level_split}

A first strategy to split the sentences is to suppose that the first level at which a separation of the sentence occurs defines the various domains. The purpose of the task is thus to find the first internal node that has more than one child, and take each child as a different domain. From each sub-tree, we look for interesting elements, nouns and verb atoms and take them as the object.

While this first attempt at splitting the tree is simple and does not take into account the subtleties of the resulting parse tree, it gives a good base line and provides a jumping point from which we can devise better techniques.

% subsection first_level_split (end)

\subsection{Clauses Extraction} % (fold)
\label{sub:clauses_extraction}

Given the fact that sentences are most of the time organized in subject-verb-object form, and that the object is often the one that has the most chances of having a subjunctive or relative clause, we can expect the tree to be leaning most of the time on the right, a fact that the previous technique will not take into account.

In order to remediate that, a second technique has been implemented, where the tree is visited in a depth-first, left-to-right manner, buffering elements in a domain until a new clause is encountered. Such a clause is encountered when we find an internal node which bears one of the following label: S, SQ, SBAR, SBARQ, FRAG (refer to Table~\ref{tab:penntree_symbols} for an overview of the symbols and to Section~\ref{sec:query_analysis} for an in-depth explanation).

From each buffered part, we then take all the leaves and filter out the ones that are neither nouns nor verbs, and send those as the resulting objects.

% subsection clauses_extraction (end)

% section sentence_splitting_strategies (end)

\section{Domain Extraction Strategies} % (fold)
\label{sec:domain_extraction_strategies}

Following the extraction of the parts of the sentences that correspond to domains, it is time to extract the actual domains from them. In order to do that, we use external bases of data, mostly centered around the tools provided by WordNet.

\subsection{Parsing WordNet} % (fold)
\label{sub:parsing_wordnet}

Given a lack of a library that was both stable and gave use internal access to the unique identifier of the synonym sets of WordNet, the work of parsing the WordNet database file had to be done within our task. Thankfully, WordNet is organized around text files, where each entry span exactly one line. For each part of the speech (noun, verb, etc.), two files are given: the first one is an index where the key is the word we are looking for, and for which the values are a list of offsets within the other file, the database, corresponding each to one different signification or sense of the word.

This second file further contains, for each signification, a definition and an example use. In addition, it contains the relationships between the different synsets.

Another important part of WordNet is the normalization of the words stored in the index. All of them are stored in their singular, lowercase format. The different words in a collocation are also separated by the underscore character.

To transform a word into a singular or infinitive form, a list of common transformations is given and tried until a match is found. A third file also lists exceptional cases, for example irregular verbs (e.g. \emph{are}\ will point to \emph{be}).

Instead, the WordNet Domains database takes form of another index, where there, the keys correspond to the offsets of the synsets within the WordNet Database. The values are there a space-separated list of the different domains the synset take part in, in order of importance. These domains are taken from the DEWEY system.

% subsection parsing_wordnet (end)

\subsection{WordNet Domains with tf-idf} % (fold)
\label{sub:wordnet_domains_with_tf_idf}

Given the access to the WordNet Database, we can now use the objects found within the previous phase to find out the domains to which they relate. In order to do this, we take each object, normalize it and look for all its senses within WordNet. Given each unique sense, we then find out all the domains written within the WordNet Domains file. This gives us, for a single part of sentence, a wealth of domains, not all of which present the same amount of information, given that some are very generic.

In order to present the most important ones, we use the information retrieval technique known as tf-idf\footnote{See http://en.wikipedia.org/wiki/tf-idf}. This technique allows us to calculate how much information carries a word within a set of document. It has two principal components. The Inverse Document Frequency, or idf, takes into account that a word that is very common within is a whole collection does not give much information, as it lacks uniqueness. So points are removed from a word if it appears in too many documents. On the other hand, a word is considered as important to a document the more times it appears it, which the tf or Total Frequency indicator purports to calculate.

These two results are combined to give a final score to each unique word within the part of the sentence, which we then use to order the results, starting from the highest score.

% subsection wordnet_domains_with_tf_idf (end)

\subsection{WordNet Topics} % (fold)
\label{sub:wordnet_topics}

Another source of data we can make use of is that of the special relationship within WordNet itself that corresponds to the semantic \emph{topic}. This relationship groups synsets under a limited amount of other words which are then considered as topics, and thus as domains we wish to extract.

% subsection wordnet_topics (end)

% section domain_extraction_strategies (end)

% chapter implementation (end)